{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharth-iyer1/460J-Labs/blob/main/DSL_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E--IYPybViN7"
      },
      "source": [
        "# Data Science Lab: Lab 5\n",
        "\n",
        "Submit:\n",
        "1. A pdf of your notebook with solutions.\n",
        "2. A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
        "\n",
        "# Goals of this Lab\n",
        "\n",
        "1. Random Forests\n",
        "2. Boosting\n",
        "3. Playing with Ensembling packages, including XGBoost and CatBoost\n",
        "4. One more time: Revisiting CIFAR-10 and MNIST\n",
        "5. Getting ready for Kaggle\n",
        "\n",
        "We will soon open a Kaggle competition made for this class. In that one, you will be participating on your own. This is an intro to get us started, and also an excuse to work with regularization and regression which we have been discussing. You'll revisit some problems from earlier labs, this time using Random Forests, and Boosting. In particular, you should take this opportunity to become familiar with some very useful packages for boosting. I recommend not only the boosting packages in scikit-learn, but also XGBoost, GBM Light, CatBoost and possibly others. You have to download these and get them running, and then read their documentation to figure out how they work, what the hyperparameters are, etc.\n",
        "\n",
        "Also, the metric we will use in the Kaggle competition is AUC. We will discuss this. In the meantime, you may want to understand how it works. At least one key thing to remember: to get a good AUC score, you need to submit a soft score (probabilities) and not rounded values (i.e., not 0s and 1s).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegd555UZRJe"
      },
      "source": [
        "## Problem 1: Revisiting Logistic Regression and MNIST\n",
        "\n",
        "We have played with the handwriting recognition problem (the MNIST data set) using decision trees. We have also considered the same problem using multi-class Logistic Regression in a previous Lab. We revisit this one more time.\n",
        "\n",
        "**Part 1**: Use Random Forests to try to get the best possible *test accuracy* on MNIST. This involves getting acquainted with how Random Forests work, understanding their parameters, and therefore using Cross Validation to find the best settings. How well can you do? You should use the accuracy metric, since this is what you used in the previous Lab  -- therefore this will allow you to compare your results from Random Forests with your results from L1- and L2- Regularized Logistic Regression.\n",
        "\n",
        "What are the hyperparameters of your best model?\n",
        "\n",
        "**Part 2**: Use Boosting to do the same. Take the time to understand how XGBoost works (and/or other boosting packages available -- CatBoost is also another favorite). Try your best to tune your hyper-parameters. As added motivation: typically the winners and near-winners of the Kaggle competition are those that are best able to tune and cross validate XGBoost. What are the hyperparameters of your best model?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MNIST Data Download\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9653142857142857\n"
          ]
        }
      ],
      "source": [
        "# MNIST Random Forest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Global Hyperparameters\n",
        "TEST_SIZE = 0.25\n",
        "NUM_ESTIMATORS = 100\n",
        "\n",
        "# Setup train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=TEST_SIZE, random_state=42)\n",
        "\n",
        "# Baseline Random Forest\n",
        "clf = RandomForestClassifier(n_estimators=NUM_ESTIMATORS, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Scores: [0.966      0.96628571 0.96809524 0.96933333 0.964     ]\n",
            "Mean: 0.9667428571428573\n",
            "Standard Deviation: 0.0018343163721910364\n"
          ]
        }
      ],
      "source": [
        "# Run Cross Validation to Optimize Hyperparameters\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Cross Validation\n",
        "scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "print(f\"Cross Validation Scores: {scores}\")\n",
        "print(f\"Mean: {np.mean(scores)}\")\n",
        "print(f\"Standard Deviation: {np.std(scores)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter Optimization\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# set bootstrap to false because in all other cases, when changing other hyperparams, bootstrap was false\n",
        "param_grid = [\n",
        "    {'n_estimators': [100, 150, 200], 'max_features': [6, 7, 8, 10], 'bootstrap': [False]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\", return_train_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
              "             param_grid=[{&#x27;bootstrap&#x27;: [False], &#x27;max_features&#x27;: [6, 8, 10],\n",
              "                          &#x27;n_estimators&#x27;: [100, 150, 200]}],\n",
              "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
              "             param_grid=[{&#x27;bootstrap&#x27;: [False], &#x27;max_features&#x27;: [6, 8, 10],\n",
              "                          &#x27;n_estimators&#x27;: [100, 150, 200]}],\n",
              "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
              "             param_grid=[{'bootstrap': [False], 'max_features': [6, 8, 10],\n",
              "                          'n_estimators': [100, 150, 200]}],\n",
              "             return_train_score=True, scoring='accuracy')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found by grid search: {'bootstrap': False, 'max_features': 10, 'n_estimators': 200}\n",
            "Best scores found by grid search: 0.9696380952380952\n",
            "Accuracy: 0.9689142857142857\n",
            "Precision: 0.9687101334449796\n",
            "Recall: 0.9685573192546881\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "predictions = grid_search.predict(X_test)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "\n",
        "print(\"Best parameters found by grid search:\", best_params)\n",
        "print(\"Best scores found by grid search:\", best_scores)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate precision\n",
        "# Note: For multiclass classification, you need to set the 'average' parameter.\n",
        "# Example: average='macro' calculates metrics for each label, and finds their unweighted mean. It does not take label imbalance into account.\n",
        "# For binary classification, you can omit the 'average' parameter or set it to 'binary' (default).\n",
        "precision = precision_score(y_test, predictions, average='macro')  # Change 'macro' as needed\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "recall = recall_score(y_test, predictions, average='macro')  # Change 'macro' as needed\n",
        "print(f\"Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_boost = y_train.astype(int)\n",
        "y_test_boost = y_test.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=2,\n",
              "             estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None,...\n",
              "                                     max_cat_threshold=None,\n",
              "                                     max_cat_to_onehot=None,\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=None, ...),\n",
              "             param_grid={'learning_rate': [0.1], 'max_depth': [6, 7],\n",
              "                         'n_estimators': [200]},\n",
              "             return_train_score=True, scoring='accuracy')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Part 2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [200],\n",
        "    'max_depth': [6,7],\n",
        "    'learning_rate': [0.1]\n",
        "}\n",
        "\n",
        "# Create the XGBoost model object\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search2 = GridSearchCV(xgb_model, param_grid, cv=2, scoring='accuracy', return_train_score=True)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search2.fit(X_train, y_train_boost)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found by grid search: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n",
            "Best scores found by grid search: 0.9691809523809525\n",
            "Accuracy: 0.9745714285714285\n"
          ]
        }
      ],
      "source": [
        "predictions2 = grid_search2.predict(X_test)\n",
        "\n",
        "best_params2 = grid_search2.best_params_\n",
        "best_scores2 = grid_search2.best_score_\n",
        "\n",
        "\n",
        "print(\"Best parameters found by grid search:\", best_params2)\n",
        "print(\"Best scores found by grid search:\", best_scores2)\n",
        "\n",
        "accuracy = accuracy_score(y_test_boost, predictions2)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyFVq4efZ33D"
      },
      "source": [
        "## Problem 2: Revisiting Logistic Regression and CIFAR-10\n",
        "\n",
        "Now that you have your pipeline set up, it should be easy to apply the above procedure to CIFAR-10. If you did something that takes significant computation time, keep in mind that CIFAR-10 is a few times larger.\n",
        "\n",
        "**Part 1**: What is the best accuracy you can get on the test data, by tuning Random Forests? What are the hyperparameters of your best model?\n",
        "\n",
        "**Part 2**: What is the best accuracy you can get on the test data, by tuning XGBoost? What are the hyperparameters of your best model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Boris\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Baseline RandomForest Accuracy on subset: 0.31\n"
          ]
        }
      ],
      "source": [
        "# Part 1\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Sample a smaller subset for faster execution\n",
        "# Adjust the test_size to change the subset size, e.g., 0.02 for 2%\n",
        "X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, test_size=0.98, random_state=42)\n",
        "X_test_sub, _, y_test_sub, _ = train_test_split(X_test, y_test, test_size=0.98, random_state=42)\n",
        "\n",
        "# Flatten the images\n",
        "X_train_flat = X_train_sub.reshape(X_train_sub.shape[0], -1)\n",
        "X_test_flat = X_test_sub.reshape(X_test_sub.shape[0], -1)\n",
        "\n",
        "# Flatten the labels\n",
        "y_train_flat = y_train_sub.ravel()\n",
        "y_test_flat = y_test_sub.ravel()\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "y_pred = clf.predict(X_test_flat)\n",
        "\n",
        "accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "print(f\"Baseline RandomForest Accuracy on subset: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
            "Best parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best cross-validation score: 0.312\n",
            "Test set accuracy of the best model on subset: 0.31\n"
          ]
        }
      ],
      "source": [
        "# TUNING on the subset\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],  \n",
        "    'max_depth': [None, 10],  \n",
        "    'min_samples_split': [2, 4],  \n",
        "}\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Setup GridSearchCV with fewer CV folds and adjusted parameters\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=2, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_flat)\n",
        "test_accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "print(f\"Test set accuracy of the best model on subset: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline XGBoost Accuracy: 0.32\n"
          ]
        }
      ],
      "source": [
        "# Part 2\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "xgb_clf.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb = xgb_clf.predict(X_test_flat)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_xgb = accuracy_score(y_test_flat, y_pred_xgb)\n",
        "print(f\"Baseline XGBoost Accuracy: {accuracy_xgb}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "Best parameters: {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Best cross-validation score: 0.32199265133396876\n",
            "Test set accuracy of the best XGBoost model: 0.3\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100],  # Number of trees\n",
        "    'learning_rate': [0.1, 0.01],  # Step size shrinkage used to prevent overfitting\n",
        "    'max_depth': [6, 10],  # Maximum depth of a tree\n",
        "    'subsample': [0.8, 1],  # Subsample ratio of the training instances\n",
        "    'colsample_bytree': [0.8, 1],  # Subsample ratio of columns when constructing each tree\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb_clf, param_grid=param_grid_xgb, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search_xgb.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(f\"Best parameters: {grid_search_xgb.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search_xgb.best_score_}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "best_model_xgb = grid_search_xgb.best_estimator_\n",
        "y_pred_best_xgb = best_model_xgb.predict(X_test_flat)\n",
        "test_accuracy_best_xgb = accuracy_score(y_test_flat, y_pred_best_xgb)\n",
        "print(f\"Test set accuracy of the best XGBoost model: {test_accuracy_best_xgb}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6XFMTH2aCRm"
      },
      "source": [
        "## Problem 3: Revisiting Kaggle\n",
        "\n",
        "This is a continuation of Problem 2 from Lab 3. You already did some first steps there, including making a Kaggle account, and trying ridge and lasso linear regression. You also tried stacking.\n",
        "\n",
        "**Part 1** (Nothing to hand in) Revisit Lab 3 and your answers there.\n",
        "\n",
        "**Part 2**: Train a gradient boosting regression, e.g., using XGBoost. What score can you get just from a single XGB? (you will need to optimize over its parameters).\n",
        "\n",
        "**Part 3**: Do your best to get a more accurate model. Try feature engineering and stacking many models. You are allowed to use any public tool in python. No non-python tools allowed.\n",
        "\n",
        "**Part 4**: (Optional)  Read the Kaggle forums, tutorials and Kernels in this competition. This is an excellent way to learn. Include in your report if you find something in the forums you like, or if you made your own post or code post, especially if other Kagglers liked or used it afterwards.\n",
        "\n",
        "**Other**: Be sure to read and learn the rules of Kaggle! No sharing of code or data outside the Kaggle forums. Every student should have their own individual Kaggle account and teams can be formed in the Kaggle submissions with your Lab partner. This is more important for live competitions of course.\n",
        "\n",
        "In the real in-class Kaggle competition (which will be next), you will be graded based on your public score (include that in your report) and also on the creativity of your solution. In your report, due after the competition closes, you will explain what worked and what did not work. Many creative things will not work, but you will get partial credit for developing them. You can start thinking about this now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "GKQMEKaHRQxg"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "\n",
        "# Load the training and testing datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Extract and remove the 'Id' column from the test dataset\n",
        "ids = test_df.pop('Id')\n",
        "\n",
        "# Combine training and testing data for consistent preprocessing\n",
        "all_data = pd.concat((train_df.loc[:, 'MSSubClass':'SaleCondition'],\n",
        "                      test_df.loc[:, 'MSSubClass':'SaleCondition']))\n",
        "\n",
        "# Set the figure size for matplotlib plots\n",
        "\n",
        "# Plot the distribution of the SalePrice and its logarithm to check skewness\n",
        "\n",
        "# Apply log transformation to the SalePrice to reduce skewness\n",
        "train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n",
        "\n",
        "# Identify numeric features from all data\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "\n",
        "# Compute skewness of numeric features and apply log transformation to skewed features\n",
        "skewed_feats = train_df[numeric_feats].apply(lambda x: skew(x.dropna()))  # Compute skewness\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75]  # Identify features with skewness > 0.75\n",
        "skewed_feats = skewed_feats.index  # Get the names of skewed features\n",
        "\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])  # Apply log1p transformation to skewed features\n",
        "\n",
        "# Convert categorical variables into dummy/indicator variables and fill missing values with column mean\n",
        "all_data = pd.get_dummies(all_data)\n",
        "all_data = all_data.fillna(all_data.mean())\n",
        "\n",
        "# Split the combined data back into training and testing datasets\n",
        "X_train = all_data[:train_df.shape[0]]\n",
        "X_test = all_data[train_df.shape[0]:]\n",
        "y = train_df.SalePrice  # Target variable for the training data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=TEST_SIZE, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=XGBRegressor(base_score=None, booster=None,\n",
              "                                    callbacks=None, colsample_bylevel=None,\n",
              "                                    colsample_bynode=None,\n",
              "                                    colsample_bytree=None, device=None,\n",
              "                                    early_stopping_rounds=None,\n",
              "                                    enable_categorical=False, eval_metric=None,\n",
              "                                    feature_types=None, gamma=None,\n",
              "                                    grow_policy=None, importance_type=None,\n",
              "                                    interaction_constraints=None,\n",
              "                                    learning_rate=None, m...=None,\n",
              "                                    max_cat_threshold=None,\n",
              "                                    max_cat_to_onehot=None, max_delta_step=None,\n",
              "                                    max_depth=None, max_leaves=None,\n",
              "                                    min_child_weight=None, missing=nan,\n",
              "                                    monotone_constraints=None,\n",
              "                                    multi_strategy=None, n_estimators=None,\n",
              "                                    n_jobs=None, num_parallel_tree=None,\n",
              "                                    random_state=None, ...),\n",
              "             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n",
              "                         'max_depth': [7, 10, 15], 'n_estimators': [200, 300]})"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Part 2\n",
        "xgb_model_kaggle = xgb.XGBRegressor()\n",
        "param_grid = {\n",
        "    'n_estimators': [200,300],\n",
        "    'max_depth': [7,10,15],\n",
        "    'learning_rate': [.01, 0.1,.2]\n",
        "}\n",
        "\n",
        "grid_search_kaggle = GridSearchCV(xgb_model_kaggle, param_grid, cv=5)\n",
        "grid_search_kaggle.fit(X_train,y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE for train:5.913801238832761e-06\n",
            "MSE for test:0.019767494333076403\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "train_predict = grid_search_kaggle.predict(X_train)\n",
        "test_predict = grid_search_kaggle.predict(X_test)\n",
        "final_predictions = np.expm1(test_predict)  # Apply expm1 to invert the log1p transformation\n",
        "mse_train = mean_squared_error(y_train,train_predict)\n",
        "mse_test = mean_squared_error(y_test,test_predict)\n",
        "print(\"MSE for train:\" + str(mse_train))\n",
        "print(\"MSE for test:\" + str(mse_test))\n",
        "#output = pd.DataFrame({'Id': ids, 'SalePrice': final_predictions.squeeze()})\n",
        "\n",
        "#Got score on kaggle of .13754\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StackingRegressor(estimators=[('xgb',\n",
              "                               XGBRegressor(base_score=None, booster=None,\n",
              "                                            callbacks=None,\n",
              "                                            colsample_bylevel=None,\n",
              "                                            colsample_bynode=None,\n",
              "                                            colsample_bytree=None, device=None,\n",
              "                                            early_stopping_rounds=None,\n",
              "                                            enable_categorical=False,\n",
              "                                            eval_metric=None,\n",
              "                                            feature_types=None, gamma=None,\n",
              "                                            grow_policy=None,\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_ra...\n",
              "                                               grow_policy=None,\n",
              "                                               importance_type=None,\n",
              "                                               interaction_constraints=None,\n",
              "                                               learning_rate=None, max_bin=None,\n",
              "                                               max_cat_threshold=None,\n",
              "                                               max_cat_to_onehot=None,\n",
              "                                               max_delta_step=None,\n",
              "                                               max_depth=None, max_leaves=None,\n",
              "                                               min_child_weight=None,\n",
              "                                               missing=nan,\n",
              "                                               monotone_constraints=None,\n",
              "                                               multi_strategy=None,\n",
              "                                               n_estimators=None, n_jobs=None,\n",
              "                                               num_parallel_tree=None,\n",
              "                                               random_state=None, ...))"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Part 3\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "best_boost = xgb.XGBRegressor(**grid_search_kaggle.best_params_)\n",
        "best_boost.fit(X_train,y_train)\n",
        "\n",
        "# Initialize additional regression models\n",
        "rf_reg = RandomForestRegressor()\n",
        "\n",
        "# Create a stack of models\n",
        "stacked_reg = StackingRegressor(\n",
        "    estimators=[('xgb', best_boost), ('rf', rf_reg)],\n",
        "    final_estimator=xgb.XGBRegressor()\n",
        ")\n",
        "\n",
        "stacked_reg.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE for train:0.012235727885252784\n",
            "MSE for test:0.029993867462066205\n"
          ]
        }
      ],
      "source": [
        "train_predict = stacked_reg.predict(X_train)\n",
        "test_predict = stacked_reg.predict(X_test)\n",
        "final_predictions = np.expm1(test_predict)  # Apply expm1 to invert the log1p transformation\n",
        "mse_train = mean_squared_error(y_train,train_predict)\n",
        "mse_test = mean_squared_error(y_test,test_predict)\n",
        "print(\"MSE for train:\" + str(mse_train))\n",
        "print(\"MSE for test:\" + str(mse_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
