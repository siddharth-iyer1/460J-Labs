{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharth-iyer1/460J-Labs/blob/main/DSL_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Lab: Lab 5\n",
        "\n",
        "Submit:\n",
        "1. A pdf of your notebook with solutions.\n",
        "2. A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
        "\n",
        "# Goals of this Lab\n",
        "\n",
        "1. Random Forests\n",
        "2. Boosting\n",
        "3. Playing with Ensembling packages, including XGBoost and CatBoost\n",
        "4. One more time: Revisiting CIFAR-10 and MNIST\n",
        "5. Getting ready for Kaggle\n",
        "\n",
        "We will soon open a Kaggle competition made for this class. In that one, you will be participating on your own. This is an intro to get us started, and also an excuse to work with regularization and regression which we have been discussing. You'll revisit some problems from earlier labs, this time using Random Forests, and Boosting. In particular, you should take this opportunity to become familiar with some very useful packages for boosting. I recommend not only the boosting packages in scikit-learn, but also XGBoost, GBM Light, CatBoost and possibly others. You have to download these and get them running, and then read their documentation to figure out how they work, what the hyperparameters are, etc.\n",
        "\n",
        "Also, the metric we will use in the Kaggle competition is AUC. We will discuss this. In the meantime, you may want to understand how it works. At least one key thing to remember: to get a good AUC score, you need to submit a soft score (probabilities) and not rounded values (i.e., not 0s and 1s).\n"
      ],
      "metadata": {
        "id": "E--IYPybViN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Revisiting Logistic Regression and MNIST\n",
        "\n",
        "We have played with the handwriting recognition problem (the MNIST data set) using decision trees. We have also considered the same problem using multi-class Logistic Regression in a previous Lab. We revisit this one more time.\n",
        "\n",
        "**Part 1**: Use Random Forests to try to get the best possible *test accuracy* on MNIST. This involves getting acquainted with how Random Forests work, understanding their parameters, and therefore using Cross Validation to find the best settings. How well can you do? You should use the accuracy metric, since this is what you used in the previous Lab  -- therefore this will allow you to compare your results from Random Forests with your results from L1- and L2- Regularized Logistic Regression.\n",
        "\n",
        "What are the hyperparameters of your best model?\n",
        "\n",
        "**Part 2**: Use Boosting to do the same. Take the time to understand how XGBoost works (and/or other boosting packages available -- CatBoost is also another favorite). Try your best to tune your hyper-parameters. As added motivation: typically the winners and near-winners of the Kaggle competition are those that are best able to tune and cross validate XGBoost. What are the hyperparameters of your best model?\n"
      ],
      "metadata": {
        "id": "iegd555UZRJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Revisiting Logistic Regression and CIFAR-10\n",
        "\n",
        "Now that you have your pipeline set up, it should be easy to apply the above procedure to CIFAR-10. If you did something that takes significant computation time, keep in mind that CIFAR-10 is a few times larger.\n",
        "\n",
        "**Part 1**: What is the best accuracy you can get on the test data, by tuning Random Forests? What are the hyperparameters of your best model?\n",
        "\n",
        "**Part 2**: What is the best accuracy you can get on the test data, by tuning XGBoost? What are the hyperparameters of your best model?"
      ],
      "metadata": {
        "id": "UyFVq4efZ33D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Revisiting Kaggle\n",
        "\n",
        "This is a continuation of Problem 2 from Lab 3. You already did some first steps there, including making a Kaggle account, and trying ridge and lasso linear regression. You also tried stacking.\n",
        "\n",
        "**Part 1** (Nothing to hand in) Revisit Lab 3 and your answers there.\n",
        "\n",
        "**Part 2**: Train a gradient boosting regression, e.g., using XGBoost. What score can you get just from a single XGB? (you will need to optimize over its parameters).\n",
        "\n",
        "**Part 3**: Do your best to get a more accurate model. Try feature engineering and stacking many models. You are allowed to use any public tool in python. No non-python tools allowed.\n",
        "\n",
        "**Part 4**: (Optional)  Read the Kaggle forums, tutorials and Kernels in this competition. This is an excellent way to learn. Include in your report if you find something in the forums you like, or if you made your own post or code post, especially if other Kagglers liked or used it afterwards.\n",
        "\n",
        "**Other**: Be sure to read and learn the rules of Kaggle! No sharing of code or data outside the Kaggle forums. Every student should have their own individual Kaggle account and teams can be formed in the Kaggle submissions with your Lab partner. This is more important for live competitions of course.\n",
        "\n",
        "In the real in-class Kaggle competition (which will be next), you will be graded based on your public score (include that in your report) and also on the creativity of your solution. In your report, due after the competition closes, you will explain what worked and what did not work. Many creative things will not work, but you will get partial credit for developing them. You can start thinking about this now."
      ],
      "metadata": {
        "id": "l6XFMTH2aCRm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKQMEKaHRQxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}